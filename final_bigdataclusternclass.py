# -*- coding: utf-8 -*-
"""Final-BigDataCLusternClass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11KEzkYRGfGjJdNVuLQ1PQaKzxPipLZOd
"""

!pip install scikit-learn

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('combined-data.csv')

# Preprocessing
data = data.drop(['userId', 'userSessionId'], axis=1)  # Remove unnecessary columns
data = data.dropna()  # Remove rows with missing values

# Separate features and target variable
X = data.drop('Player-type', axis=1)
y = data['Player-type']

# Encoding categorical variables
X_encoded = pd.get_dummies(X)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

k = 3  # Set the value of k for k-NN
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the KNN model
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = knn.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Read the dataset from CSV
data = pd.read_csv('combined-data.csv')

# Preprocess the data
data = data.dropna()  # Remove rows with missing values
X = data.drop(['Player-type'], axis=1)  # Features
y = data['Player-type']  # Target variable

# Convert categorical variables to numeric using LabelEncoder
le = LabelEncoder()
X['platformType'] = le.fit_transform(X['platformType'])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the K-Nearest Neighbors classifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Make predictions on the test set
y_pred = knn.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import OneHotEncoder

# Read the dataset from CSV file
data = pd.read_csv('combined-data.csv')

# Encode categorical variables using one-hot encoding
categorical_cols = ['platformType']
encoder = OneHotEncoder(sparse=False)
encoded_cols = pd.DataFrame(encoder.fit_transform(data[categorical_cols]))
encoded_cols.columns = encoder.get_feature_names_out(categorical_cols)

# Combine encoded columns with the original data
data_encoded = pd.concat([data, encoded_cols], axis=1)
data_encoded.drop(categorical_cols, axis=1, inplace=True)

# Split the data into features and target variable
X = data_encoded.drop('Player-type', axis=1)
y = data_encoded['Player-type']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)
lr_accuracy = accuracy_score(y_test, lr_predictions)
print("Logistic Regression Accuracy:", lr_accuracy)
print(classification_report(y_test, lr_predictions))

# Support Vector Machines
svm_model = SVC()
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_predictions)
print("Support Vector Machines Accuracy:", svm_accuracy)
print(classification_report(y_test, svm_predictions))

# Naive Bayes
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
nb_predictions = nb_model.predict(X_test)
nb_accuracy = accuracy_score(y_test, nb_predictions)
print("Naive Bayes Accuracy:", nb_accuracy)
print(classification_report(y_test, nb_predictions))

# K-Nearest Neighbors
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
knn_predictions = knn_model.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_predictions)
print("K-Nearest Neighbors Accuracy:", knn_accuracy)
print(classification_report(y_test, knn_predictions))

import matplotlib.pyplot as plt

# Define the metrics for Logistic Regression
lr_precision_n = 0.91
lr_recall_n = 0.85
lr_f1_n = 0.88
lr_precision_p = 0.89
lr_recall_p = 0.94
lr_f1_p = 0.92

# Define the metrics for Naive Bayes
nb_precision_n = 0.83
nb_recall_n = 0.96
nb_f1_n = 0.89
nb_precision_p = 0.97
nb_recall_p = 0.86
nb_f1_p = 0.91

# Create the line plot
labels = ['Precision-NOOB', 'Recall-NOOB', 'F1-score-NOOB', 'Precision-PRO', 'Recall-PRO', 'F1-score-PRO']
logistic_vals = [lr_precision_n, lr_recall_n, lr_f1_n, lr_precision_p, lr_recall_p, lr_f1_p]
naive_bayes_vals = [nb_precision_n, nb_recall_n, nb_f1_n, nb_precision_p, nb_recall_p, nb_f1_p]

x = range(len(labels))

plt.plot(x, logistic_vals, marker='o', label='Logistic Regression', color='purple')
plt.plot(x, naive_bayes_vals, marker='o', label='Naive Bayes', color='green')

plt.xlabel('Yardstick')
plt.ylabel('Outcomes')
plt.title('Logistic Regression and Naive Bayes')
plt.xticks(x, labels, rotation=45)
plt.legend()

# Add value annotations on the markers
for i, val in enumerate(logistic_vals):
    plt.text(i, val, f"{val:.2f}", ha='center', va='bottom')

for i, val in enumerate(naive_bayes_vals):
    plt.text(i, val, f"{val:.2f}", ha='center', va='bottom')

plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read the data from CSV
df = pd.read_csv('combined-data.csv')

# Encode categorical variables
df_encoded = pd.get_dummies(df, columns=['platformType', 'Player-type'])

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_encoded)

# Apply K-means clustering
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(df_scaled)

# Add the cluster labels to the DataFrame
df['cluster'] = kmeans.labels_

# Print the results
print(df[['userId', 'userSessionId', 'cluster']])

import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler

# Read the data from CSV
df = pd.read_csv('combined-data.csv')

# Encode categorical variables
df_encoded = pd.get_dummies(df, columns=['platformType', 'Player-type'])

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_encoded)

# Apply K-means clustering
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(df_scaled)

# Apply Agglomerative Clustering
agg_clustering = AgglomerativeClustering(n_clusters=2)
agg_labels = agg_clustering.fit_predict(df_scaled)

# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(df_scaled)

# Add the cluster labels to the DataFrame
df['kmeans_cluster'] = kmeans.labels_
df['agg_cluster'] = agg_labels
df['dbscan_cluster'] = dbscan_labels

# Print the results
print(df[['userId', 'userSessionId', 'kmeans_cluster', 'agg_cluster', 'dbscan_cluster']])

import pandas as pd
from sklearn.cluster import KMeans

# Read the data from CSV
df = pd.read_csv('combined-data.csv')

# Extract the three numeric dimensions for clustering
data = df[['count_gameclicks', 'count_hits', 'avg_price']]

# Apply K-means clustering with 7 clusters
kmeans = KMeans(n_clusters=7, random_state=42)
kmeans.fit(data)

# Add the cluster labels to the DataFrame
df['cluster'] = kmeans.labels_

# Calculate the centroid of each cluster
centroids = kmeans.cluster_centers_

# Get the size (number of data points) in each cluster
cluster_sizes = df['cluster'].value_counts()

# Print the properties of each cluster
for cluster_label, centroid, size in zip(range(7), centroids, cluster_sizes):
    print(f"Cluster {cluster_label} - Centroid: {centroid}, Size: {size}")

import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read the data from CSV
df = pd.read_csv('combined-data.csv')

# Extract the features for clustering
features = ['count_gameclicks', 'count_hits', 'avg_price']

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[features])

# Apply K-means clustering with a chosen number of clusters
k = 7
kmeans = KMeans(n_clusters=k, random_state=42)
clustering = kmeans.fit_predict(df_scaled)

# Add the cluster labels to the dataframe
df['Cluster'] = clustering

# Calculate the centroid coordinates for each cluster
centroids = kmeans.cluster_centers_

# Create a 3D scatter plot
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')

# Set background color to black
fig.patch.set_facecolor('black')
ax.set_facecolor('black')

# Plot the data points in 3D space
for cluster in range(k):
    cluster_points = df[df['Cluster'] == cluster]
    ax.scatter(cluster_points['count_gameclicks'], cluster_points['count_hits'], cluster_points['avg_price'],
               label=f'Cluster {cluster}', c=f'C{cluster}')

# Plot the centroids in 3D space
ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], marker='x', color='white', s=40, label='Centroids')

# Set labels and title with white color
ax.set_xlabel('Count-Game-clicks', color='white')
ax.set_ylabel('Count-Hits', color='white')
ax.set_zlabel('Average-Price', color='white')
ax.set_title('Clustering K-means for Centroids', color='white')

# Set legend color to white
legend = ax.legend(loc='upper left')
legend.get_frame().set_facecolor('black')
for text in legend.get_texts():
    text.set_color('white')

# Set tick label colors to white
ax.tick_params(axis='x', colors='white')
ax.tick_params(axis='y', colors='white')
ax.tick_params(axis='z', colors='white')

# Set axes grid color to white
ax.xaxis._axinfo['grid']['color'] = 'white'
ax.yaxis._axinfo['grid']['color'] = 'white'
ax.zaxis._axinfo['grid']['color'] = 'white'

# Adjust the aspect ratio of the plot
ax.set_box_aspect([1, 1, 1])  # Adjust the third value as needed

# Show the 3D scatter plot
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read the data from CSV
df = pd.read_csv('combined-data.csv')

# Extract the features for clustering
features = ['count_gameclicks', 'count_hits', 'avg_price']

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[features])

# Apply K-means clustering with a chosen number of clusters
k = 7
kmeans = KMeans(n_clusters=k, random_state=42)
clustering = kmeans.fit_predict(df_scaled)

# Add the cluster labels to the dataframe
df['Cluster'] = clustering

# Calculate the mean values for each feature per cluster
cluster_means = df.groupby('Cluster')[features].mean()

# Convert the cluster means to a numpy array
cluster_means_array = cluster_means.values

# Normalize the values to be in the range [0, 1]
cluster_means_normalized = (cluster_means_array - cluster_means_array.min(axis=0)) / (cluster_means_array.max(axis=0) - cluster_means_array.min(axis=0))

# Define the angles for each axis
angles = np.linspace(0, 2 * np.pi, len(features), endpoint=False).tolist()

# Make a complete loop by appending the start value
angles += angles[:1]

# Create the Radar Chart
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'projection': 'polar'})
ax.set_theta_offset(np.pi / 2)
ax.set_theta_direction(-1)
plt.xticks(angles[:-1], features)
ax.set_rlabel_position(0)

# Plot each cluster
for i in range(k):
    values = cluster_means_normalized[i].tolist()
    values += values[:1]
    ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'Cluster {i}')

# Fill the area of each cluster
for i in range(k):
    values = cluster_means_normalized[i].tolist()
    values += values[:1]
    ax.fill(angles, values, alpha=0.3)

# Add legend and title
plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
plt.title('Cluster Profiles')

# Show the Radar Chart
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read the data from CSV
df = pd.read_csv('combined-data.csv')

# Extract the features for clustering
features = ['count_gameclicks', 'count_hits', 'avg_price']

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[features])

# Apply K-means clustering with a chosen number of clusters
k = 7
kmeans = KMeans(n_clusters=k, random_state=42)
clustering = kmeans.fit_predict(df_scaled)

# Add the clustering values to the dataframe
df['Clustering'] = clustering

# Print the clustering value for each data point
print(df['Clustering'])

# Output:
# 0       1
# 1       0
# 2       0
# 3       1
# 4       1
#       ...
# 3452    4
# 3453    2
# 3454    2
# 3455    2
# 3456    2
# Name: Clustering, Length: 3457, dtype: int32