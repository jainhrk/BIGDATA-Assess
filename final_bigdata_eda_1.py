# -*- coding: utf-8 -*-
"""Final-Bigdata-EDA-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14tL_Yd2zivd9ZUG1MdRz6JqJAm7kTzOj
"""

from google.colab import files
uploaded = files.upload()
import sys, os
sys.path
sys.executable

!apt-get install openjdk-8-jdk-headless

!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz

!tar xf /content/spark-3.2.1-bin-hadoop2.7.tgz

!pip install -q findspark
!pip install pyspark

import os
os.environ["JAVA_HOME"] =  "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop2.7"
import findspark
findspark.init()
findspark.find()

import pyspark
sc = pyspark.SparkContext(master="local[*]", appName="FirstExample")

sc.setLogLevel("ERROR")

from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local").appName("assessment_bigdata").config("spark.ui.port", "4050").getOrCreate()

spark

buy_clicks_df = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)

buy_clicks_df.printSchema()

buy_clicks_df.show(5)

buy_clicks_df.describe().toPandas().transpose()

############################### EXPLORATORY DATA ANALYSIS

users_df = spark.read.csv("/content/users.csv", header=True, inferSchema=True)
team_df = spark.read.csv("/content/team.csv", header=True, inferSchema=True)
adclicks_df = spark.read.csv("/content/ad-clicks.csv", header=True, inferSchema=True)
buyclicks_df = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)

users_df.show(10)

###### CONVERT DOB TO AGE

from pyspark.sql import functions as f



# Resource for below Code : https://stackoverflow.com/questions/62148704/date-difference-in-years-in-pyspark-dataframe

users_with_age_df = users_df.withColumn("age",   (f.months_between(f.current_date(), f.col('dob')) / 12).cast('int'))

users_with_age_df.show(5)

# list(users_with_age_df.toPandas()["age"].values)
users_with_age_df.show()

team_df = spark.read.csv("/content/team.csv", header=True, inferSchema=True)
buyclicks_df = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)


team_buying_df = team_df.alias("teamdf").join(buyclicks_df.alias("buyclicks"),\
                            f.col("teamdf.teamId") == f.col("buyclicks.team"),"inner")

team_buying_df_grouped = team_buying_df.groupBy("teamId").sum("price", "strength")\
         .select('teamId', f.col('sum(price)').alias('total_spending'), f.col('sum(strength)').alias('total_strength'))\
         .orderBy("sum(price)")

!pip install scipy

from matplotlib import patches
from scipy.spatial import ConvexHull
import numpy as np
import seaborn as sns
import warnings; warnings.simplefilter('ignore')
sns.set_style("white")

# Step 1: Prepare Data
df = team_buying_df_grouped.toPandas() 

# As many colors as there are unique midwest['category']
categories = np.unique(df['teamId'])
colors = [plt.cm.tab10(i/float(len(categories)-1)) for i in range(len(categories))]

# Step 2: Draw Scatterplot with unique color for each category
fig = plt.figure(figsize=(16, 10), dpi= 80, facecolor='w', edgecolor='k')    


# Step 3: Draw Scatterplot

ax = sns.scatterplot(data=df, x="total_spending", y="total_strength", hue="teamId",size="teamId", legend="full")
m, b = np.polyfit(df["total_spending"], df["total_strength"], 1)





# Step 3.1 : Set Legend
# plt.setp(ax.get_legend().get_texts(), fontsize='7') # for legend text
# plt.setp(ax.get_legend().get_title(), fontsize='7') # for legend title

# Step 4: Decorations

plt.xticks(fontsize=12); plt.yticks(fontsize=12)
plt.xlabel("Team Spending")
plt.ylabel("Team Strength")
plt.legend(title='Team Ids', fontsize=7)

plt.title("Scatter Plot of team total spending vs team strength", fontsize=22)

plt.show()   

team_buying_df_grouped.stat.corr("total_spending","total_strength")

!pip install pycountry_convert

############# TEAM SPENDING WITH AGE GROUP
team_user_buying_df = team_buying_df.alias("team_buying_df").join(users_df.alias("users_df"),\
                           f.col("team_buying_df.userId") == f.col("users_df.userId"),"inner")

import pycountry_convert as pc
from pyspark.sql.types import StringType
def country_to_continent(country_name):
  try:
    country_continent_code = pc.country_alpha2_to_continent_code(country_name)
    country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)
    return country_continent_name
  except:
    return "None"
udf_country_to_continent = f.udf(country_to_continent , StringType())
team_user_buying_df = team_user_buying_df.withColumn("continent", udf_country_to_continent(f.col("country") ))
team_user_buying_df.show()

!pip install squarify

!pip install pywaffle

team_user_buying_df_stat.show()

!pip install pywaffle

import matplotlib.pyplot as plt
import pandas as pd

# Read the data into a Pandas DataFrame.
df = pd.read_csv("/content/ad-clicks.csv", header=True, inferSchema=True)

# Group the data by the product ID and calculate the total spending.
df = df.groupby("productId").agg(sum("price"))

# Plot the data as a line chart.
plt.plot(df.index, df["sum(price)"])
plt.xlabel("Product ID")
plt.ylabel("Total Spending")
plt.title("Highest Sold Products")
plt.show()

import seaborn as sns
import pyspark.sql.functions as f
adclicks_df = spark.read.csv("/content/ad-clicks.csv", header=True, inferSchema=True)



adclicks_df_stat = adclicks_df.groupBy("adCategory").agg(f.count("adCategory").alias("count")).orderBy("count").toPandas()
                                            


# users_with_age_df.toPandas().hist(column = "age", bins = 10)
import matplotlib.pyplot as plt
import seaborn as sns

import random




# Plot Bars
plt.figure(figsize=(16,10), dpi= 80)
ax = sns.barplot(x="adCategory", y="count", data=adclicks_df_stat,

                 palette="Blues_d")


plt.title("Popular Ad Genre", fontsize=22)
plt.ylabel('Ad Count')
plt.xlabel('Ad Type')
plt.show()

############## GET DEVICE PIE CHART
import matplotlib.pyplot as plt
import seaborn as sns
user_session_df = spark.read.csv("/content/user-session.csv", header=True, inferSchema=True)
platform_usage_stats_df = user_session_df.groupBy("platformType").agg(f.mean("teamLevel").alias("average_rank"), 
                                            f.count("platformType").alias("total_platform_count")).orderBy("average_rank")
xlabels = list(platform_usage_stats_df.toPandas()["platformType"])
# platform_usage_stats_df.toPandas().plot.bar()
labels = list(platform_usage_stats_df.toPandas()["platformType"])
sizes =  list(platform_usage_stats_df.toPandas()["total_platform_count"])
colors = ['skyblue', 'pink', 'purple','yellow', 'orange','orange','pink', "silver"]
# explode = tuple( if X == 0 else x=0 for x in range(len(labels)) ) 
max = 0
for i in sizes:
  if max< i:
    max = i
  

explode = list(0 for _ in sizes)
explode[sizes.index(max)]  = 0.1
# 0, 0.1, 0, 0,0,0
# Plot
plt.figure(figsize=(10,8))
plt.pie(sizes,explode = explode, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=140)
# Create a circle for the center of the plot
my_circle=plt.Circle( (0,0), 0.7, color="white")
p=plt.gcf()
p.gca().add_artist(my_circle)
plt.axis('equal')
plt.show()





######################## CHECKING HIGHER RANK TEAMS PLAY WHICH EVENTS
######################### GAME CLICKS IS PLATFORM DEPENDANT

game_clicks_df = spark.read.csv("/content/game-clicks.csv", header=True, inferSchema=True)
user_session_df = spark.read.csv("/content/user-session.csv", header=True, inferSchema=True)



game_clicks_user_session_df = game_clicks_df.alias("game_clicks_df").join(user_session_df.alias("user_session_df"), 
                                            f.col("game_clicks_df.userid") == f.col("user_session_df.userId"), "inner")\
                                            


# https://stackoverflow.com/questions/49021972/pyspark-count-rows-on-condition




game_clicks_user_session_pf_df = game_clicks_user_session_df.groupby("platformType")\
                  .agg( f.sum( f.when(f.col("isHit") == 1, 1).otherwise(0)).alias("platform_hit_count") , 
                  f.count("isHit").alias("total_hits"))\
                  .withColumn("percentage", f.col("platform_hit_count") / f.col("total_hits"))\
                  .orderBy("platform_hit_count")
                  
ylabels = list(game_clicks_user_session_pf_df.toPandas()["platformType"]  )    
                  


game_clicks_user_session_pf_df.toPandas().plot(
                   kind='barh',
                  figsize=(12,8),
                  logx=True).set_yticklabels(ylabels)

# game_clicks_user_session_df.groupby("platformType").agg( f.count("isHit") )
# game_clicks_user_session_pf_df.show()

# game_clicks_user_session_pf_df_percentage = game_clicks_user_session_pf_df\
                                            # .withColumn("percentage",f.col("platform_hit_count").divide(f.col("total_hits")))


# game_clicks_user_session_pf_df_percentage

game_clicks_user_session_pf_df.show()

import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style="whitegrid")

# Initialize the matplotlib figure
f, ax = plt.subplots(figsize=(20, 8))

# Load the data
platform_types = list(map(str, game_clicks_user_session_pf_df.toPandas()["platformType"]))
total_hits = list(game_clicks_user_session_pf_df.toPandas()["total_hits"])
platform_hit_counts = list(game_clicks_user_session_pf_df.toPandas()["platform_hit_count"])

# Sort the data in descending order
sorted_data = sorted(zip(total_hits, platform_types, platform_hit_counts), reverse=True)
total_hits, platform_types, platform_hit_counts = zip(*sorted_data)

# Create stacked area chart for "Total clicks"
sns.set_color_codes("pastel")
sns.lineplot(x=total_hits, y=platform_types, marker='o', markersize=5, label="Overall clicks", color="g")
plt.fill_between(total_hits, platform_types, color="g", alpha=0.3)

# Create stacked area chart for "Strike counts"
sns.set_color_codes("muted")
sns.lineplot(x=platform_hit_counts, y=platform_types, marker='o', markersize=5, label="Strike counts", color="pink")
plt.fill_between(platform_hit_counts, platform_types, color="pink", alpha=0.3)

# Add labels to the data points
for x, y, label in zip(total_hits, platform_types, total_hits):
    ax.text(x + 500, y, str(label), ha='left', va='center', color='black', fontweight='bold')

for x, y, label in zip(platform_hit_counts, platform_types, platform_hit_counts):
    ax.text(x + 200, y, str(label), ha='left', va='center', color='black', fontweight='bold')

# Set the axis labels and title
ax.set(ylabel="Source Type", xlabel="Counts", title="Strike's Count v/s Overall Clicks")

# Add a legend and remove spines
ax.legend(ncol=2, loc="lower right", frameon=True)
sns.despine(left=True, bottom=True)

plt.show()

############# CORR
from pyspark.sql import functions as f

team_df = spark.read.csv("/content/team.csv", header=True, inferSchema=True)
buyclicks_df = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)


team_buying_df = team_df.alias("teamdf").join(buyclicks_df.alias("buyclicks"),\
                            f.col("teamdf.teamId") == f.col("buyclicks.team"),"inner")



########## HISTOGRAM OF SPENDING COUNT HOW MUCH SPENDING IS DISTRIBUTED && CORRELATION

# team_buying_df.groupBy("teamId").sum("price", "strength").orderBy("sum(price)").toPandas().hist(column = "sum(price)")


team_buying_df_grouped = team_buying_df.groupBy("teamId").sum("price", "strength")\
         .select('teamId', f.col('sum(price)').alias('total_spending'), f.col('sum(strength)').alias('total_strength'))\
         .orderBy("sum(price)")

team_buying_df.stat.corr("price","strength")
# team_buying_df_grouped.show()