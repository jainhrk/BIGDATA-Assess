# -*- coding: utf-8 -*-
"""Final-Bigdata-EDA-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q1q5iQl0ceirerd4fEh28kE6l0BlwDgw
"""

!pip install findspark
!pip install pyspark

from pyspark.sql import SparkSession
import matplotlib.pyplot as plt
spark = SparkSession.builder \
    .appName("EDA Visualization") \
    .getOrCreate()

import matplotlib.pyplot as plt
from pyspark.sql.functions import sum, col

item_purchases_stat_df = buy_clicks_df.groupby("buyId").agg(sum(col("price")).alias("total_price")).orderBy("buyId").toPandas()

plt.figure(figsize=(16, 10), dpi=80, facecolor='w', edgecolor='k')
plt.plot(item_purchases_stat_df["buyId"], item_purchases_stat_df["total_price"], marker="o", linestyle="-")
plt.xticks(rotation=0)
#plt.title("Spending by Product", loc='left')
plt.xlabel('BUY-ID')
plt.ylabel('Spendings')

# Display values on the plot
for x, y in zip(item_purchases_stat_df["buyId"], item_purchases_stat_df["total_price"]):
    plt.text(x, y, f"{y}", ha='center', va='bottom')

plt.show()

from pyspark.sql import SparkSession
import plotly.graph_objects as go

# Create a SparkSession
spark = SparkSession.builder \
    .appName("EDA Visualization") \
    .getOrCreate()

# Define the data schema and load the data into a Spark DataFrame
from pyspark.sql.types import StructType, StructField, StringType

# Define the data schema
schema = StructType([
    StructField("timestamp", StringType(), True),
    StructField("userId", StringType(), True),
    StructField("nick", StringType(), True),
    StructField("twitter", StringType(), True),
    StructField("dob", StringType(), True),
    StructField("country", StringType(), True),
    StructField("Weekday", StringType(), True),
    StructField("Month", StringType(), True),
    StructField("Time Period", StringType(), True)
])

# Load the data into a DataFrame
data = spark.read.csv("users.csv", header=True, schema=schema)

# Convert the date of birth column to date type
from pyspark.sql.functions import to_date, year, current_date
data = data.withColumn("dob", to_date("dob", "dd-MM-yyyy"))

# Calculate age based on the date of birth
data = data.withColumn("age", year(current_date()) - year("dob"))

# Group age into 8 gaps
from pyspark.sql.functions import floor
data = data.withColumn("age_group", floor(data["age"] / 10) * 10)

# Perform grouping and aggregation to get the count for each age group
age_group_counts = data.groupBy("age_group").count().orderBy("age_group").collect()

# Prepare the data for the donut plot
age_groups = [str(row["age_group"]) for row in age_group_counts]
user_counts = [row["count"] for row in age_group_counts]

# Create the donut plot
fig = go.Figure()
fig.add_trace(go.Pie(
    labels=age_groups,
    values=user_counts,
    hole=0.4,  # Set the size of the inner hole for the donut effect
    name="User Count",
))
fig.update_traces(hoverinfo='label+percent+value')

fig.update_layout(
    title="Donut Plot of User Count by Age Group",
    height=600,
    width=800,
)

fig.show()

import plotly.graph_objects as go

# Prepare the data for the funnel chart
time_periods = data.select("Time Period").distinct().toPandas()["Time Period"]
time_period_counts = data.groupBy("Time Period").count().orderBy("Time Period").toPandas()

# Create the funnel chart
fig = go.Figure(go.Funnel(
    y=time_periods,
    x=time_period_counts["count"],
    textposition="inside",
    textinfo="value+percent total",
))

fig.update_layout(
    title="Users-Count by Time-Period",
    height=600,
    width=800,
)

fig.show()


fig.show()

import plotly.graph_objects as go
import pandas as pd

# Prepare the data for the Parallel Categories Diagram
age_groups = [str(row["age_group"]) for row in age_group_counts]
user_counts = [row["count"] for row in age_group_counts]

# Define the 8 age group categories with gaps
age_group_categories = ["0-8", "9-16", "17-24", "25-32", "33-40", "41-48", "49-56", "57-64","65+"]

# Categorize the age groups based on the defined categories
age_group_categories = pd.cut(pd.Series(age_groups).astype(int), bins=[0, 8, 16, 24, 32, 40, 48, 56, 64,  100], labels=age_group_categories)

# Create the Parallel Categories Diagram
fig = go.Figure(data=[go.Parcats(
    dimensions=[
        {"label": "Age-Range", "values": age_group_categories},
        {"label": "Weekday", "values": data.select("Weekday").toPandas()["Weekday"]},
        {"label": "Users-Count", "values": user_counts}
    ],
    counts=user_counts,
    line={"color": user_counts, "colorscale": "Viridis"}
)])

fig.update_layout(
    title="",
    height=600,
    width=800,
)

fig.show()

from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType, StringType, StructField, StructType
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans

# Create a Spark session
spark = SparkSession.builder.appName("ClusteringAnalysis").getOrCreate()

# Define the schema for the DataFrame
schema = StructType([
    StructField("userId", IntegerType(), True),
    StructField("userSessionId", IntegerType(), True),
    StructField("teamLevel", IntegerType(), True),
    StructField("platformType", StringType(), True),
    StructField("count_gameclicks", IntegerType(), True),
    StructField("count_hits", IntegerType(), True),
    StructField("count_buyId", StringType(), True),
    StructField("avg_price", StringType(), True),
    StructField("Player-type", StringType(), True)
])

# Read data from CSV file into a DataFrame
csv_file_path = "/content/combined-data.csv"
df = spark.read.csv(csv_file_path, header=True, schema=schema)

# Perform feature vectorization
assembler = VectorAssembler(inputCols=['teamLevel', 'count_gameclicks', 'count_hits'], outputCol='features')
vectorized_df = assembler.transform(df)

# Perform clustering using K-means algorithm
kmeans = KMeans(k=2, seed=42)
model = kmeans.fit(vectorized_df)

# Add cluster labels to the DataFrame
clustered_df = model.transform(vectorized_df)

# Print the resulting clusters
clustered_df.select('userId', 'prediction').show()
# Assuming you have already performed clustering and have the 'clustered_df' DataFrame

from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.clustering import KMeans

# Create a Spark session
spark = SparkSession.builder.appName("PlayerTypeClustering").getOrCreate()

# Read data from CSV file into a DataFrame
csv_file_path = "/content/combined-data.csv"
df = spark.read.csv(csv_file_path, header=True, inferSchema=True)

# Convert 'Player-type' column to numeric using StringIndexer
indexer = StringIndexer(inputCol='Player-type', outputCol='label')
indexed_df = indexer.fit(df).transform(df)

# Perform feature vectorization
assembler = VectorAssembler(inputCols=['label'], outputCol='features')
vectorized_df = assembler.transform(indexed_df)

# Perform clustering using K-means algorithm
kmeans = KMeans(k=2, seed=42)
model = kmeans.fit(vectorized_df)

# Add cluster labels to the DataFrame
clustered_df = model.transform(vectorized_df)

# Print the resulting clusters
clustered_df.select('Player-type', 'prediction').show()

import numpy as np

assignments_count_pd = assignments_count.toPandas()

# Sort the DataFrame by teamId
assignments_count_pd.sort_values(by="teamId", inplace=True)

# Define custom colors for the bars
colors = ["orange", "green", "red", "purple"]

# Create a stacked bar chart with custom colors
ax = assignments_count_pd.plot(kind="bar", x="teamId", stacked=True, color=colors)

# Increase the size of the x-axis
fig = plt.gcf()
fig.set_size_inches(10, 6)  # Adjust the figure size as per your requirement

# Customize the x-axis tick labels
ax.set_xticks(np.arange(len(assignments_count_pd)))
ax.set_xticklabels(assignments_count_pd["teamId"])

plt.xlabel("Team")
plt.ylabel("Assignment-Count")
plt.title("Assignment Count per Team")
plt.legend(title="Assignment Type")
plt.show()

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
df = spark.read.csv("team.csv", header=True, inferSchema=True)
df.show()
df.printSchema()
df.describe().show()
df = df.withColumn("teamCreationTime", df["teamCreationTime"].cast("timestamp"))
df = df.withColumn("teamEndTime", df["teamEndTime"].cast("timestamp"))
df = df.withColumn("strength", df["strength"].cast("double"))
df = df.withColumn("currentLevel", df["currentLevel"].cast("integer"))

import matplotlib.pyplot as plt

# Convert Spark DataFrame to Pandas DataFrame
df_pd = df.toPandas()

# Plot the bar plot
counts, bins, patches = plt.hist(df_pd["strength"], bins=10, alpha=0.5, color="brown")

plt.xlabel("Strength")
plt.ylabel("Count")
plt.title("Distribution of Strength")

# Add lines between the bars
for i in range(len(counts) - 1):
    plt.vlines(bins[i + 1], 0, counts[i], color='black', linestyle='-')

# Add value annotations on top of each bar
for count, patch in zip(counts, patches):
    plt.text(patch.get_x() + patch.get_width() / 2, patch.get_height(), str(int(count)),
             ha='center', va='bottom')

plt.show()



# Example 2: Box plot
#df_pd.boxplot(column=["strength"], medianprops={"color": "pink"})
#plt.ylabel("Strength")
#plt.title("Box Plot of Strength")
#plt.show()

df_pd.boxplot(column=["strength"], medianprops={"color": "pink"})

# Get the median value
median_value = df_pd["strength"].median()

# Add the median value as text on the plot
plt.text(0.95, median_value, str(round(median_value, 2)), color="black", ha='left')

plt.ylabel("Strength")
plt.title("Box Plot of Strength")
plt.show()

import matplotlib.pyplot as plt

# Box plot of count_gameclicks by Player-type
df_pd = df.toPandas()

# Box plot with median value
box = plt.boxplot([df_pd[df_pd["Player-type"] == "NOOB"]["count_gameclicks"],
                   df_pd[df_pd["Player-type"] == "PRO"]["count_gameclicks"]],
                  labels=["NOOB", "PRO"],
                  showfliers=False,  # Exclude outliers
                  medianprops={'color': 'pink', 'linewidth': 2})  # Customize median line

plt.xlabel("Player-type")
plt.ylabel("Game-clicks-count")
plt.title("Game-clicks-Count by Player-type")

# Add median value annotations
for median in box['medians']:
    median_x = median.get_xdata().mean()
    median_y = median.get_ydata().mean()
    plt.text(median_x, median_y, f"{median_y:.1f}",
             ha='center', va='bottom', color='white')

plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Read the CSV file into a pandas DataFrame
df = pd.read_csv('buy-clicks.csv')

# Create a figure and axis
fig, ax = plt.subplots(figsize=(8, 6))

# Box plot for 'price'
price_box = ax.boxplot(df['price'], positions=[0], widths=0.6, labels=['Price'])

# Box plot for 'buyId'
buyid_box = ax.boxplot(df['buyId'], positions=[1], widths=0.6, labels=['Buy ID'])

# Set y-axis label and title
ax.set_ylabel('PRICE')
ax.set_title('Price and Buy ID Plot')

# Adjust x-axis ticks
ax.set_xticks([0, 1])
ax.set_xticklabels(['Price', 'Buy ID'])

# Function to add median values to the box plot
def add_median_values(box):
    for line in box['medians']:
        # Get the coordinates of the median line
        x, y = line.get_xydata()[1]
        # Add the median value as text
        plt.text(x, y, f"{y:.2f}", ha='right', va='bottom', color='black')

# Add median values to the price box plot
add_median_values(price_box)

# Add median values to the buyId box plot
add_median_values(buyid_box)

# Show the plot
plt.show()